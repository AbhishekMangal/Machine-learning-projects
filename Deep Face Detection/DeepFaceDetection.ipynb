{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9a6c7-2fa7-4349-89d2-88a7a97a0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import uuid \n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39855c6-80bf-4984-9556-d1d396cc2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid.uuid1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c44281-9d73-4234-a3e7-4b4022fc85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('D:\\\\data', 'images')  # final path: D:\\images\\images\n",
    "number_images = 60\n",
    "if not os.path.exists(IMAGES_PATH):\n",
    "    os.makedirs(IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4462544-7fe8-4967-8d11-8d52c40b6dc1",
   "metadata": {},
   "source": [
    "# Annotate Images with LabelMe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027324d3-3509-45d4-8443-a451709715ae",
   "metadata": {},
   "source": [
    "# Review Dataset and Build Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f167b44-3e6d-423d-9900-3ef81b6e2ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "for imgnum in range(number_images):\n",
    "    print('Collecting image{}'.format(imgnum))\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame!\")\n",
    "        continue  # Skip this iteration\n",
    "\n",
    "    imgname = os.path.join(IMAGES_PATH, f'{str(uuid.uuid1())}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    time.sleep(1)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a33ad1-fa9d-41fb-ba74-812c80869691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!labelme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429dc8bf-5f38-4039-bd32-7b9f2cb5b733",
   "metadata": {},
   "source": [
    "### Partition augmentaiton data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb761c-19b6-420d-9fd2-d58f09efbae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ae364-6648-429b-b9d7-fc0fd1b28d31",
   "metadata": {},
   "source": [
    "Limit Gpu Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0af9c-deca-4477-a4be-092a941f7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f40ac-902e-462d-b730-9f860caeb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afcf84-b04f-41ea-b35b-588c110b3dbd",
   "metadata": {},
   "source": [
    "Load Image into Tf data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f5574-3cf1-49ea-8918-75429e7185bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files('D:/data/images/*.jpg', shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ffddd4-56d3-4acf-bf85-a548b7e75cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c8d593-fa52-44ba-b9a6-6b5c67262d39",
   "metadata": {},
   "source": [
    "view Raw images with matplotlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdbe560-0509-4a8c-9892-5febcff609ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x):\n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img, channels=3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5eda1-70f9-43bb-8b6d-f38384d77ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8abc31-9ecd-41ff-864f-c7ae233a094f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a64e8-31de-4209-b602-ae1755f6950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc36418-2fac-485c-84b5-820fbdab3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_images = image_generator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a19beec-e0f3-496d-b2dc-d979ca0ee588",
   "metadata": {},
   "source": [
    "Partition Augemntaed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26249ca1-e5d2-4f8a-8143-9868169cae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee767db9-29b4-4236-9874-9fda13db38f8",
   "metadata": {},
   "source": [
    "Manuallu split data into train test and val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79054c31-9290-43a1-b23d-d5f2d435ce80",
   "metadata": {},
   "source": [
    "Make the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb48e9-37eb-40fc-a2fb-7e47bd54bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dir = 'D:/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b30aff-ebed-4f4a-8a30-9e4fa2e3e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "# Create necessary folders if they don't exist\n",
    "for folder in ['train', 'test', 'val']:\n",
    "    image_folder = os.path.join(base_dir, folder, 'images')\n",
    "    label_folder = os.path.join(base_dir, folder, 'labels')\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "    os.makedirs(label_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b5ea09-d12f-43c3-a60b-15c52d1c998a",
   "metadata": {},
   "source": [
    "Shuffle the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c6a18-7f26-44d6-84a1-3cb38d71bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = os.listdir(os.path.join(base_dir, 'images'))\n",
    "random.shuffle(all_images)  # Shuffle for randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4c8ff-06cb-4e7e-bd59-9f9a7473d7eb",
   "metadata": {},
   "source": [
    "Split the data and define data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cd56a-ef83-440e-9613-0d129fcbabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "# Calculate split sizes\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "\n",
    "total_images = len(all_images)\n",
    "train_count = int(total_images * train_ratio)\n",
    "val_count = int(total_images * val_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b687b57-42e0-4c3d-970e-42ab58220e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = all_images[:train_count]\n",
    "val_files = all_images[train_count:train_count + val_count]\n",
    "test_files = all_images[train_count + val_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544db9c-7521-4156-95e4-af8097d0ab0e",
   "metadata": {},
   "source": [
    "Function to move file in train test and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77144cc5-8ab9-4a37-991e-1e8f3100ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_files(file_list, split_name):\n",
    "    for file in file_list:\n",
    "        filename_wo_ext = os.path.splitext(file)[0] + '.json'\n",
    "        \n",
    "        # Paths for image\n",
    "        src_image = os.path.join(base_dir, 'images', file)\n",
    "        dst_image = os.path.join(base_dir, split_name, 'images', file)\n",
    "        \n",
    "        # Copy image (always)\n",
    "        shutil.copy2(src_image, dst_image)\n",
    "        \n",
    "        # Paths for label\n",
    "        src_label = os.path.join(base_dir, 'labels', filename_wo_ext)\n",
    "        dst_label = os.path.join(base_dir, split_name, 'labels', filename_wo_ext)\n",
    "        \n",
    "        # Copy label only if exists\n",
    "        if os.path.exists(src_label):\n",
    "            shutil.copy2(src_label, dst_label)\n",
    "        else:\n",
    "            print(f\"Label missing for {file}, only image copied.\")\n",
    "\n",
    "# Copy files to respective folders\n",
    "copy_files(train_files, 'train')\n",
    "copy_files(val_files, 'val')\n",
    "copy_files(test_files, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d838a-d88b-4abc-8da9-49dbf057c005",
   "metadata": {},
   "source": [
    "# Apply Image Augmentaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accea32c-295f-4c68-bc4b-c61a8d6bd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecd8db-0e1e-49d3-9b86-e96b5885443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65404f9b-6b35-4f43-b855-602d984db96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = cv2.imread(os.path.join(base_dir,'train', 'images','3f096c72-4f84-11f0-9d23-f43bd8788907.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41975647-4591-40cc-aa55-0d333d558c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(base_dir, 'train', 'labels', '3f096c72-4f84-11f0-9d23-f43bd8788907.json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded0a114-981a-4b2e-bc17-735b9615b601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15efcf1-7adb-4317-984a-70a755079c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6f456-29f4-4d4b-8f97-fb5995043ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b1d55-c650-4ef5-a98d-29064b869dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8949b-5cc8-46f7-9171-10c72063c3e2",
   "metadata": {},
   "source": [
    "# Extract Coordinates and Resclae to Match Image Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e010b-6fbb-4954-a6d6-4729dac0ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As Batch Size = 4 so need corrdinates of 4 images\n",
    "coords= [0,0,0,0]\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "coords[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93e3da-f88d-4917-bf03-11fe68dc6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3be25-0079-4e60-ab90-f0979e235cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scalind\n",
    "coords = list(np.divide(coords, [640,480,640,480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef897f6c-5767-4768-948c-a6029f2148fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681911ba-9934-494c-a88c-def872dd5149",
   "metadata": {},
   "source": [
    "# Apply Augmentaion and View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f64301-b81b-493c-b3b7-ffb0a5f56d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image = img, bboxes=[coords], class_labels = ['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44f8de-b252-4bf4-9f52-5aebd76696cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c91c4a-a877-416a-8e71-e4a8a9bacaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6854a7a-3e4f-4496-a978-8cc1f041fc26",
   "metadata": {},
   "source": [
    "Build and run augmentaion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2880b0d7-fc71-4f6f-a24f-cbf3bf53246e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'],\n",
    "             tuple(np.multiply(augmented['bboxes'][0][:2], [450, 450]).astype(int)),\n",
    "             tuple(np.multiply(augmented['bboxes'][0][2:], [450, 450]).astype(int)),\n",
    "              (250,250,0),2)\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02574c5f-fd6a-47df-81b6-09bb0867ed29",
   "metadata": {},
   "source": [
    "# Step 5 Build and Run Augmentaion Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1933845-b4de-4c7a-8ab2-504398f9831b",
   "metadata": {},
   "source": [
    "5.1 Run augmentaion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c7ad6-ed43-43ee-9a5e-18c5e323a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary folders if they don't exist\n",
    "for folder in ['train', 'test', 'val']:\n",
    "    image_folder = os.path.join('D:/aug_data', folder, 'images')\n",
    "    label_folder = os.path.join('D:/aug_data', folder, 'labels')\n",
    "    os.makedirs(image_folder, exist_ok=True)\n",
    "    os.makedirs(label_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9210de-fc89-4d5f-929a-034db3b7e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train', 'test', 'val']:\n",
    "    for image in os.listdir(os.path.join(base_dir, partition, 'images')):\n",
    "        img = cv2.imread(os.path.join(base_dir, partition,'images', image))\n",
    "       \n",
    "        coords = [0,0,0.00001, 0.00001] #for if label doesn't exist\n",
    "        label_path = os.path.join(base_dir, partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path,'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, (640,480,640,480)))\n",
    "\n",
    "        try:\n",
    "            for x in range(60): #creating 60 augmented image for single image\n",
    "                augmented =augmentor(image = img ,bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('D:/aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if(len(augmented['bboxes'])  == 0):\n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0\n",
    "                    else:\n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "\n",
    "                else :\n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0\n",
    "                with open(os.path.join('D:/aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc853d9-0e0f-48a4-96da-1313f110e198",
   "metadata": {},
   "source": [
    "5.2 Load Augmented images to tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebe9a8-c414-402a-8313-6bd151984ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files(\"D:/aug_data/train/images/*.jpg\" , shuffle = False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf498416-4358-45a6-ac44-0264d93698e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files(\"D:/aug_data/test/images/*.jpg\" , shuffle = False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a43acb-769b-4346-9c96-2af0273f623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files(\"D:/aug_data/val/images/*.jpg\" , shuffle = False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1143bd-1e4b-4c92-8789-89a3b9aae2d9",
   "metadata": {},
   "source": [
    "# 6 Prepare Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d67c19-9cfa-4bb6-90f5-a22b98d8744b",
   "metadata": {},
   "source": [
    "6.1 Build Label Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a05db0-6a67-4b9f-ad08-7795811b8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = 'utf-8') as f:\n",
    "        label = json.load(f)\n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df77e7-cc9f-4dcd-8f3a-cab43588967e",
   "metadata": {},
   "source": [
    "6.2 Load Label nto tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5003c77-45fa-48da-bcc7-5f50a2757773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files(\"D:/aug_data/train/labels/*.json\",shuffle = False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x],[tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8210b-b1e3-43c9-909c-89038461f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files(\"D:/aug_data/test/labels/*.json\",shuffle = False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x],[tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8fcea-b63e-4769-8a08-a4e2e282fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files(\"D:/aug_data/val/labels/*.json\",shuffle = False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x],[tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5b616-18e1-4a38-91d6-26ceeba342dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa13354-0282-4d82-ad61-0ed511b3d96c",
   "metadata": {},
   "source": [
    "# 7 Combined Load and Image sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33b506-75b3-4d89-bea4-614b268c9b26",
   "metadata": {},
   "source": [
    "7.1 Check Partition Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ecbb1-40bd-46dc-be21-b697b01937ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1628ce-363a-4e93-9f8c-e094f49a8af3",
   "metadata": {},
   "source": [
    "7.2 Create final Datasets (images/labelx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4042574-0566-47b2-9b15-7ad3fbaec7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f56b86-ec7e-4d49-b1cb-e2685d47c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(5000)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a1042-e2b4-4780-a1dc-9fac34fb7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(5000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86991263-e597-4035-b92a-2ee9aa433456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29630e5d-8f85-4529-8b82-73f701c388e8",
   "metadata": {},
   "source": [
    "7.3 View Images and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872898bb-687f-4b7b-9390-bdb314465b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_samples = train.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae88d3b-6eb7-4a99-972d-9d25b3bc4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res =data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c6633-0fc1-47cc-8364-df2e2fbec118",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx].copy()\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc009358-3079-462e-acc3-e7c5a2d28529",
   "metadata": {},
   "source": [
    "# Build Deep Learning using the Fuctional Api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228775e-7703-47eb-9b81-df668f0858d5",
   "metadata": {},
   "source": [
    "8.1 Import Layers and Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedea332-c700-4e1a-8ee6-7135968db678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59a5dd-2d4e-4fa0-8f9e-fb3f22216d19",
   "metadata": {},
   "source": [
    "### Download VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b28288-2a86-4e53-a876-25dd05a8613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e2496-e71b-45be-85e1-105fc96cd125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92266e8d-c043-46b5-9c3b-05e05da5ef82",
   "metadata": {},
   "source": [
    "### 8.3 Build instance of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd7ca0-a120-4ec4-a0b0-dd6308e1913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    # Classification Model  for detect object is presnet or not\n",
    "    f1 = GlobalMaxPooling2D()(vgg)  #going from all layers of vgg\n",
    "    class1 = Dense(2048, activation='relu')(f1) \n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding box model for predicting the rectangel box coordinates\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff335dd-faaa-4062-8f1a-2f202a5c8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a8005-2a2e-44db-b546-4f444c917529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6b186-fd39-4063-a751-bdb00557d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1] #As two ouptut classes and Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1207415-fb94-4784-80ed-d83abc50b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f28bf-521e-4786-98db-e5ef2cb12125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fc9ca-d801-433c-b4ab-4ab4bb50154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape, y[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f94fdf-2bbe-4283-a6ae-f0d908a9638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes, coords = facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97feb0-1bdb-4dee-a318-647eae95a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd92fc-8a51-4809-9247-a09f1575e8d0",
   "metadata": {},
   "source": [
    "# 9 Define Losses and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a93788-3eb4-49dc-978b-4b23ba19adec",
   "metadata": {},
   "source": [
    "### 9.1 Define Optimizer and Lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200fffd-1ca5-4332-aa62-85db9d937a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1./0.75 -1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bc659-fa1f-4357-a55b-cc35971363be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay #learning reate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a9c6b-ea2d-459b-a064-30d5f3298d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16a143-9364-4c06-8b06-bcdc42f15ed8",
   "metadata": {},
   "source": [
    "### 9.2 Create Localization Loss and Classificatio Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01771a-4c23-4895-8635-9ec5f0368d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "\n",
    "    h_true = y_true[:,3]-y_true[:,1]\n",
    "    w_true = y_true[:,2]-y_true[:,0]\n",
    "\n",
    "    h_pred = yhat[:,3]- yhat[:,1]\n",
    "    w_pred = yhat[:,2]- yhat[:,0]\n",
    "\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred)+ tf.square(h_true - h_pred))\n",
    "\n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73444ad-057e-4c2b-b475-c17172635568",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bcc560-af8a-435a-8d5e-0d9e163b336b",
   "metadata": {},
   "source": [
    "### 9.3 Test Our Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e91e9-2a88-4f72-8292-86826627f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadb5d2-6897-4850-8c68-f1305b374091",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3df9ae-45b7-49d3-81c0-6b74cc7be20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regressloss(y[1], coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be76fff-67a3-444c-9113-04c84ed025c9",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c365c95-ee3b-45a4-a209-dff4df6b3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model): \n",
    "    def __init__(self, eyetracker,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs): \n",
    "        \n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "\n",
    "            #doing gradients and back proopogation\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a46e0-684c-459a-9be3-e5d30c7f9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad00396-c48b-422f-addd-91ee82fa4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4da7ba-4f71-4391-952a-d8e429fd2b37",
   "metadata": {},
   "source": [
    "### 10.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451698b-66be-43ac-85ff-5380d63363d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04e9f8-9785-4794-903a-4317d943c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95848b-e549-4d42-b367-48c3b9c0e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6d001-827f-4353-a1a8-ffa7280068d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a658fb-7b6c-49a5-9b78-7f55aa475531",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=40, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd56306-d11a-4467-92a5-8d0efe1a17be",
   "metadata": {},
   "source": [
    "### 10.3 Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0176ac-5b10-4cd3-857a-ae4f496e4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2a70b-6bf2-4bea-84f9-6b75026b42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73a8b8-0064-4ac0-948e-0e2995a04e4d",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9355d0d-f97c-4e8a-81eb-89d057e7cee3",
   "metadata": {},
   "source": [
    "### 11.1  Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fdde2-8f45-43ff-bebc-fe8b47d23ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2763e4f-d4e2-45a8-a3c7-88d72fa080c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9954b-e54f-4ddd-89b0-6cf2e95fac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0df8e-22b5-4a26-b535-37c4b30babbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx].copy()\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.5:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bd46e-a1eb-4cd2-baab-da37270f52a6",
   "metadata": {},
   "source": [
    "# 11.2 Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a468bbc-f00c-4b1d-a333-27ba41ee8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0caffbb-6fbc-406c-8d7f-c2f0de6d4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369bea3-a7d3-472e-8358-7045eed2eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facetracker.save(fr'D:/models/faceTracker/{model_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18652c25-df50-4aae-8b4e-b7382b8cf194",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model(fr'D:/models/faceTracker/{model_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25ec3d-27fd-4216-8899-e2bfa2aebfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c863049-45bd-46f4-b29d-81bab23b9537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
